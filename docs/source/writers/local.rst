Local Writer
============

The ``Local Writer`` of **WarpRec** is designed to track and persist all experiment results, trained models, and configuration details to a local source.

Writing to a Local Source
-------------------------

When an experiment starts, WarpRec automatically initializes a dedicated, timestamped directory for the run, unless explicitly disabled via configuration. This directory houses all generated outputs and follows the structure below::

    experiment_dir/
    ├── evaluation/
    ├── params/
    ├── recs/
    ├── serialized/
    ├── split/
    └── config.json

Each file and subdirectory is **timestamped** to ensure reproducibility and allow multiple runs within the same environment without overwriting results.

Directory Contents
^^^^^^^^^^^^^^^^^^

.. list-table::
   :widths: 25 75
   :header-rows: 1

   * - Directory/File
     - Description
   * - **evaluation/**
     - Contains model evaluation results, including metric scores and a detailed report with temporal and spatial information (unless disabled).
   * - **params/**
     - Stores hyperparameter configurations found during the Hyperparameter Optimization (HPO) phase.
   * - **recs/**
     - Contains the final recommendations produced by each model (if enabled).
   * - **serialized/**
     - Stores the serialized (pickled) versions of trained models (if serialization is enabled).
   * - **split/**
     - Contains the dataset splits (train/validation/test) used for the experiment (if enabled).
   * - **config.json**
     - A fully resolved configuration file for the experiment. This includes user-specified values and automatically set default parameters, ensuring precise tracking of the configuration used for each run.

Evaluation Reporting Artifacts
------------------------------

Within the ``evaluation/`` directory, WarpRec produces two primary, timestamped artifacts:

Overall Results
^^^^^^^^^^^^^^^

A tabular file (``Overall_Results``) summarizing the evaluation metrics per model and cutoff :math:`\mathbf{k}`::

    Model       Top@k   Metric_1    Metric_2    ...
    My_Model    10      0.001       0.002       ...
    ...

.. note::

    - One row is produced per :math:`\mathbf{(model, k)}` combination.
    - Metrics reflect the configured evaluation protocol (e.g., full vs. sampled).

Time Report
^^^^^^^^^^^

A timing and environment summary file (``Time_Report``) with wall-clock measurements:

* **Data Preparation Time:** End-to-end data preparation (splitting, filtering, evaluation sampling if enabled).
* **Hyperparameter Exploration Time:** Total HPO duration, including Ray cluster bootstrap/initialization overhead.
* **Average Trial Time:** Mean training time per trial, including end-of-epoch evaluation.
* **Evaluation Time:** Time to evaluate the best model; depends on the evaluation strategy (“full” or “sampled”).
* **Inference Time (ms):** Time to predict for :math:`\mathbf{1,000 users \times 1,000 items}`; intended as a comparative guideline.
* **Total Time:** Aggregate end-to-end duration for the model’s experiment pipeline.

Unless otherwise specified, durations are wall-clock; inference time is reported in milliseconds.

Dataset Split Structure
-----------------------

The ``split/`` directory contains the dataset partitions generated by WarpRec, if dataset splitting is enabled. The structure typically follows this pattern::

    experiment_dir/
    ├── split/
    |   ├── train.tsv
    |   ├── validation.tsv
    |   ├── test.tsv
    |   ├── 1/
    |   |   ├── train.tsv
    |   |   ├── validation.tsv
    |   ├── 2/
    |   |   ├── train.tsv
    |   |   ├── validation.tsv
    ...
    └── config.json

.. note::

    - The output format (e.g., **TSV**) is configurable.
    - At the **top level** of the ``split/`` directory, you will find the primary **train/validation/test split**.
    - If validation folds are generated (e.g., for cross-validation), subdirectories named ``1/``, ``2/``, etc., are created. Each contains its own **train/validation split**.
