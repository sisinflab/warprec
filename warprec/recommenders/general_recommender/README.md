# ğŸŒ General Recommenders

The `General Recommenders` module of WarpRec is a collection of `collaborative` and `content-based` models. In the following section you will find the list of available models inside WarpRec with their respective parameters. You can try them as-is, or you can personalize them to fit you experimental needs.

## ğŸ“š Table of Contents
- ğŸŒ€ [Autoencoders](#ğŸŒ€-autoencoders)
- ğŸ“„ [Content Based](#ğŸ“„-content-based)
- ğŸ•¸ï¸ [Graph Based](#ï¸ğŸ•¸ï¸-graph-based)
- ğŸ‘¥ [KNN (K Nearest Neighbor)](#ğŸ‘¥-knn-k-nearest-neighbor)
- ğŸ§Š [Latent Factor](#ğŸ§Š-latent-factor)
- ğŸ§  [Neural](#ğŸ§ -neural)
- ğŸ“Œ [Summary of Available General Models](#ğŸ“Œ-summary-of-available-general-models)

## ğŸŒ€ Autoencoders

Autoencoder models learn compact latent representations of users or items by reconstructing user-item interaction data. These models are particularly effective in sparse recommendation settings.

- [EASE (Embarrassingly Shallow Autoencoder)](autoencoder/ease.py): A simple, closed-form linear model that uses ridge regression to learn item-item similarities. Highly efficient and effective as a collaborative filtering baseline.
```yaml
models:
    EASE:
        l2: 10
...
```
- [AddEASE](autoencoder/addease.py): An extension of EASE model using side information. Solves two linear problems increasing complexity of the underlying problem. **This model needs side information to be provided**.
```yaml
models:
    AddEASE:
        l2: 10
...
```
- [CEASE](autoencoder/cease.py): An extension of EASE model using side information. Extends the EASE problem without adding more complexity. **This model needs side information to be provided**.
```yaml
models:
    CEASE:
        l2: 10
...
```
- [MultiDAE (Multinomial Denoising Autoencoder)](autoencoder/multidae.py): A deep autoencoder trained with dropout for denoising input data. Learns robust latent representations from implicit feedback using a multinomial loss.
```yaml
models:
    MultiDAE:
        intermediate_dim: 600
        latent_dim: 200
        dropout: 1.0
        epochs: 10
        learning_rate: 0.001
        l2_lambda: 0.02
...
```
- [MultiVAE (Multinomial Variational Autoencoder)](autoencoder/multivae.py): A probabilistic variant of MultiDAE that models uncertainty in user preferences via variational inference. Useful for capturing diverse user behaviors and providing more personalized recommendations.
```yaml
models:
    MultiVAE:
        intermediate_dim: 600
        latent_dim: 200
        dropout: 1.0
        epochs: 10
        learning_rate: 0.001
        l2_lambda: 0.02
        anneal_cap: 0.2
        anneal_step: 200
...
```

## ğŸ“„ Content Based

Content-based models generate recommendations by analyzing item attributes and comparing them with user preferences. These models rely on side information (e.g., tags, text descriptions, metadata) to build user profiles and match them with similar items. They're particularly useful in cold-start scenarios or when collaborative signals are sparse.

- [VSM (Vector Space Model)](content_based/vsm.py): A classical content-based recommender that represents items and users in a shared vector space. Items are encoded using TF-IDF or binary features, while user profiles are typically aggregated from consumed items. Recommendations are generated by computing similarity (e.g., cosine) between user and item vectors. **This model needs side information to be provided**.
```yaml
models:
    VSM:
        similarity: cosine
        user_profile: binary
        item_profile: tfidf
...
```

## ğŸ•¸ï¸ Graph Based

Graph-based recommenders exploit the structure of the user-item interaction graph to infer relationships and make recommendations. These models capture high-order proximity and implicit associations through walks or neighborhood propagation. They are well-suited for uncovering complex patterns in sparse datasets.

- [RP3Beta](graph_based/rp3beta.py): A graph-based collaborative filtering model that performs a biased random walk of length 3 on the user-item bipartite graph. It integrates a popularity-based penalization term (beta) to reduce the influence of popular items, resulting in more diverse and personalized recommendations.
```yaml
models:
    RP3Beta:
        k: 10
        alpha: 0.1
        beta: 0.1
        normalize: True
...
```

## ğŸ‘¥ KNN (K Nearest Neighbor)

KNN-based models generate recommendations by identifying the most similar users or items based on interaction patterns or side information. These models are simple yet effective, and their performance largely depends on the similarity function and neighborhood size.

- [ItemKNN](knn/itemknn.py): A collaborative item-based KNN model that recommends items similar to those the user has already interacted with. Similarities between items are computed using measures like cosine similarity, and the top-k neighbors are used to score candidates.
```yaml
models:
    ItemKNN:
        k: 10
        similarity: cosine
        normalize: True
...
```

- [AttributeItemKNN](knn/attributeitemknn.py): An item-based KNN variant that incorporates item content (e.g., tags, features) to compute similarities, allowing recommendations even when historical interactions are sparse or unavailable.
```yaml
models:
    AttributeItemKNN:
        k: 10
        similarity: cosine
        normalize: True
...
```

- [UserKNN](knn/userknn.py): A collaborative user-based KNN model that recommends items liked by users with similar interaction histories. The model computes pairwise similarities between users and leverages their preferences to generate recommendations.
```yaml
models:
    UserKNN:
        k: 10
        similarity: cosine
        normalize: True
...
```

- [AttributeUserKNN](knn/attributeuserknn.py): A user-based KNN model that uses content-based profiles (e.g., TF-IDF) to define user similarity. This model is particularly useful in cold-start scenarios or when interaction data is limited.
```yaml
models:
    AttributeUserKNN:
        k: 10
        similarity: cosine
        user_profile: tfidf
        normalize: True
...
```

## ğŸ§Š Latent Factor

- [ADMMSlim (Alternating Direction Method of Multipliers Sparse Linear Methods)](latent_factor/admmslim.py): An efficient implementation of SLIM using the ADMM optimization algorithm. It learns a sparse item-to-item similarity matrix for top-N recommendation, balancing interpretability and performance.
```yaml
models:
    ADMMSlim:
        lambda_1: 0.1
        lambda_2: 0.1
        alpha: 0.2
        rho: 0.35
        it: 10
        positive_only: False
        center_columns: False
...
```

- [BPR (Bayesian Personalized Ranking)](latent_factor/bpr.py): A pairwise ranking model that optimizes the ordering of items for each user. BPR is particularly effective for implicit feedback and is trained to maximize the margin between positive and negative item pairs
```yaml
models:
    BPR:
        embedding_size: 16
        epochs: 20
        learning_rate: 0.001
...
```

- [Slim (Sparse Linear Methods)](latent_factor/slim.py): A collaborative filtering model that learns a sparse item similarity matrix using L1 and L2 regularization. SLIM directly models the relationship between items, making it highly interpretable and effective for top-N recommendation
```yaml
models:
    Slim:
        l1: 0.2
        alpha: 0.1
...
```

## ğŸ§  Neural

Neural recommenders leverage deep learning architectures to model complex, non-linear interactions between users and items. These models can integrate collaborative signals with side information and are especially powerful when trained on large-scale datasets.

- [NeuMF (Neural Matrix Factorization)](neural/neumf.py): Combines Generalized Matrix Factorization (GMF) with a Multi-Layer Perceptron (MLP) to capture both linear and non-linear user-item interactions. NeuMF is a highly expressive model that can adapt to various patterns in user behavior, making it suitable for both implicit and explicit feedback scenarios.
```yaml
models:
    NeuMF:
        mf_embedding_size: 32
        mlp_embedding_size: 32
        mlp_hidden_size: [64, 32]
        mf_train: True
        mlp_train: True
        dropout: 0.01
        epochs: 20
        learning_rate: 0.001
        neg_samples: 1
...
```

## ğŸ“Œ Summary of Available General Models

In this section you can find a table summarizing all the models available inside WarpRec, with a little description.

| Category         | Model                | Description                                                       |
| ---------------- | -------------------- | ----------------------------------------------------------------- |
| ğŸŒ€ Autoencoders  | **EASE**             | Linear autoencoder using ridge regression for item similarity.    |
|                  | **AddEASE**          | EASE with side information for improved accuracy.                 |
|                  | **CEASE**            | EASE with side information for improved accuracy.                 |
|                  | **MultiDAE**         | Denoising autoencoder optimized for implicit data.                |
|                  | **MultiVAE**         | Variational autoencoder modeling uncertainty in preferences.      |
| ğŸ“„ Content Based | **VSM**              | Classical content-based model using TF-IDF and cosine similarity. |
| ğŸ•¸ï¸ Graph Based  | **RP3Beta**          | Random walk model with popularity penalization.                   |
| ğŸ‘¥ KNN           | **ItemKNN**          | Item-based collaborative KNN using similarity metrics.            |
|                  | **AttributeItemKNN** | Item-based KNN using content features.                            |
|                  | **UserKNN**          | User-based collaborative KNN using historical interactions.       |
|                  | **AttributeUserKNN** | User-based KNN using content-derived user profiles.               |
| ğŸ§Š Latent Factor | **ADMMSlim**         | Sparse item similarity model optimized via ADMM.                  |
|                  | **BPR**              | Pairwise ranking model for implicit feedback.                     |
|                  | **Slim**             | Interpretable item similarity model with L1/L2 regularization.    |
| ğŸ§  Neural        | **NeuMF**            | Hybrid neural model combining GMF and MLP layers.                 |
