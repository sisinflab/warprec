# WarpRec Documentation Generation

WarpRec is a high-performance, backend-agnostic recommendation framework designed to bridge the "Deployment Chasm" between academic rigor and industrial scale. The repository already contains a documentation draft built with Sphinx and hosted on ReadTheDocs (using reStructuredText), but the current draft is incomplete, approximative, and poorly structured. A comprehensive rewrite is required to produce a documentation suite that serves two purposes: (1) a mathematically rigorous scientific and technical reference with exhaustive taxonomies, class/method signatures, and LaTeX formulas; and (2) a professional framework advertisement that clearly articulates the Unique Selling Points of WarpRec — Green AI tracking via CodeCarbon, Agentic AI readiness via the Model Context Protocol (MCP), Ray-based distributed training, Narwhals backend-agnosticism, and statistical rigor with Bonferroni/FDR corrections.

Therefore, acting as a **Senior Technical Writer** specializing in Machine Learning and MLOps documentation, a **Recommender Systems Researcher** with deep domain expertise, and a **Developer Advocate** with marketing acumen, you are asked to address the following points:

1. **Sphinx Configuration & Toolchain Setup.** Audit and update the Sphinx configuration file (`docs/source/conf.py`) to enable automatic API documentation generation. The current configuration only includes `sphinx.ext.mathjax`. You must add the following extensions: `sphinx.ext.autodoc`, `sphinx.ext.napoleon`, `sphinx.ext.autosummary`, and `sphinx.ext.viewcode`. Configure `sys.path.insert` so that autodoc can discover the `warprec` package. Add `autodoc_mock_imports` for heavy dependencies that should not be imported during documentation builds: `torch`, `torch_geometric`, `ray`, `narwhals`, `codecarbon`, `torchmetrics`, `polars`, `pandas`. Configure Napoleon for Google-style docstrings: set `napoleon_google_docstring = True`, `napoleon_numpy_docstring = False`, `napoleon_use_param = True`, `napoleon_use_rtype = True`, and `napoleon_attr_annotations = True`. Verify that the `toctree` in `index.rst` reflects the final documentation structure produced by the subsequent tasks. Self-ask and resolve the following critical questions before proceeding:
   - WarpRec docstrings contain `$$...$$` LaTeX blocks (Markdown-style), inline `$...$` math, reStructuredText links (`` `text <url>`_ ``), Google-style parameter sections (`Args:`, `Returns:`, `Raises:`, `Attributes:`), and ASCII art tensor diagrams. Napoleon preprocesses docstrings before Sphinx renders them. Will `$$` blocks survive Napoleon preprocessing intact? What Napoleon settings are needed to prevent mangling of LaTeX content?
   - Some docstrings use `r"""` raw string literals (e.g., `nDCGRendle2020` in `warprec/evaluation/metrics/accuracy/ndcg.py`) while others do not. Determine whether this inconsistency causes problems for autodoc and document the required convention.
   - The `mathjax` extension is configured, but verify whether `mathjax_config` or `mathjax3_config` needs to be extended to recognize inline `$...$` notation (which is not enabled by default in MathJax 3).
   - Model classes are decorated with `@model_registry.register(name="...")` and metric classes with `@metric_registry.register("...")`. Verify that these decorators are transparent to autodoc and do not hide the underlying class from documentation generation.

2. **Landing Page & Architecture Overview.** Write a compelling landing page (`index.rst`) that advertises WarpRec. Highlight the core problem — the fragmented landscape between eager-execution academic tools (RecBole, Elliot, Cornac, DaisyRec) that are limited to single-node execution and rigid industrial frameworks that prioritize serving over science — and how WarpRec bridges this gap. Detail the 5 decoupled engines (Reader, Data Engine, Recommendation Engine, Evaluation, Writer) and the Application Layer (REST API & MCP server). Highlight the 4 Pillars of WarpRec: Scalability (Ray), Green AI (CodeCarbon), Agentic Readiness (MCP), and Scientific Rigor (Bonferroni/FDR corrections). Write the architecture overview page (`architecture.rst`) with a detailed description of each engine, its responsibilities, and how they interconnect. Write the introduction page (`introduction.rst`) expanding on the research paper's Section 2 (The Fragmented Landscape) to provide academic context.

3. **Installation & Quick Start.** Write the installation guide (`install.rst`) with step-by-step instructions for all supported installation methods: Poetry (recommended), pip with virtual environment, and Conda/Mamba. Document the dependency groups (`core`, `dashboard`, `remote-io`) and special installation requirements for PyTorch Geometric (PyG) graph models. Write the quick start guide (`quick-start.rst`) with three distinct, fully coded examples — each including both the complete YAML configuration file and the Python command to run it:
   - *The Academic:* A local, in-memory script using the **Design Pipeline** for rapid prototyping on a single train/test split.
   - *The Industrial:* A distributed training script using **Ray** and the **Training Pipeline** with HPO (Hyperparameter Optimization), showing Ray cluster setup and scheduler configuration.
   - *The Agentic:* A quickstart showing how to spin up the **MCP server** and query the recommender via an LLM (like Claude), including a mock dialogue example.

4. **Pipelines.** Create a new documentation section (`pipelines/`) with an index page and one sub-page per pipeline. This section must provide detailed descriptions of the three execution pipelines implemented in WarpRec, including their purpose, when to use each, and how they relate to each other. For each pipeline, provide the complete YAML configuration file, the Python invocation command, and a step-by-step walkthrough of the execution flow referencing the corresponding source code:
   - **Design Pipeline** (`pipelines/design.rst`): A simplified pipeline for rapid prototyping and debugging. Runs locally without Ray, uses a single train/test split, does not perform HPO, and does not invoke the Writer module. Source: `warprec/pipelines/design.py`. Invocation: `python -m warprec.run -c config.yml -p design`.
   - **Training Pipeline** (`pipelines/train.rst`): The main experimental pipeline for full-scale experiments. Supports HPO via Ray Tune with configurable search spaces and schedulers (ASHA), cross-validation, statistical significance testing between models, model serialization, and time reporting. Source: `warprec/pipelines/train.py`. Invocation: `python -m warprec.run -c config.yml -p train`. Document the three execution flows: train/test, train/val/test, and cross-validation. Include Ray cluster setup instructions.
   - **Evaluation Pipeline** (`pipelines/eval.rst`): An evaluation-only pipeline that does not train models. It can either (a) load pre-trained model checkpoint files via `torch.load()` and `load_state_dict()`, or (b) read precomputed recommendation files produced by WarpRec or other frameworks (via `ProxyRecommender`), and evaluate them using the evaluation module. Supports statistical significance computation. Source: `warprec/pipelines/eval.py`. Invocation: `python -m warprec.run -c config.yml -p eval`. Provide YAML configuration examples for both checkpoint-based and file-based evaluation.
   - **Pipeline Design Guide** (`pipelines/guide.rst`): A guide explaining how to design new custom pipelines using WarpRec's modular engines. Include best practices (callback hooks, deterministic seeding, checkpoint strategy) and common pitfalls to avoid.

5. **Evaluating External Recommendations.** Create a dedicated cross-cutting documentation page (`evaluation/proxy_evaluation.rst`) that explains how to evaluate precomputed recommendation files produced by WarpRec or external frameworks (e.g., Elliot, RecBole, Cornac). This feature is currently described under the Unpersonalized Recommenders section, but it is a cross-cutting concern that applies to all recommendation types and pipelines — not just unpersonalized models. Move the detailed documentation of `ProxyRecommender` out of `recommenders/unpersonalized.rst` (leave only a brief mention with a cross-reference) and into this new dedicated page. Provide YAML configuration examples for: (a) evaluating a single external recommendation file, (b) evaluating multiple recommendation files from a folder, (c) evaluating external recommendations alongside native WarpRec models for direct comparison, and (d) using the Evaluation Pipeline to evaluate recommendations from other frameworks. Note that `ProxyRecommender` must execute locally and raises an error in distributed Ray scenarios. Cross-reference this page from the Evaluation Pipeline documentation (Task 4) and the unpersonalized recommenders taxonomy (Task 7). Use the Elliot documentation page `elliot/docs/source/guide/proxy_model.rst` as a structural reference for this section.

6. **Metrics Taxonomy.** Restructure the metrics documentation (`evaluation/metrics/`) to follow the pattern established by Elliot's `elliot/docs/source/guide/metrics_intro.rst`. Create a master index page (`evaluation/metrics/index.rst`) with a `.. py:module:: warprec.evaluation.metrics` directive, a `.. autosummary::` block listing `base_metric.BaseMetric`, and 8 family sections — each with its own `.. autosummary::` block listing all metrics in that family. For each of the 8 metric families, create (or update the existing) individual sub-pages using the Elliot `elliot/docs/source/guide/metrics/accuracy.rst` pattern — a Summary section with `.. autosummary::`, followed by individual metric entries with `.. autoclass:: MetricName :show-inheritance:`. The 8 families and their metrics are:
   - **Accuracy** (12): Precision, Recall, nDCG, nDCGRendle2020, HitRate, MAP, MAR, MRR, F1, AUC, GAUC, LAUC
   - **Rating** (3): MAE, MSE, RMSE
   - **Coverage** (4): ItemCoverage, UserCoverage, UserCoverageAtN, NumRetrieved
   - **Novelty** (2): EFD, EPC
   - **Diversity** (3): GiniIndex, ShannonEntropy, SRecall
   - **Bias** (5): ARP, ACLT, APLT, PopRSP, PopREO
   - **Fairness** (9): REO, RSP, BiasDisparityBD, BiasDisparityBR, BiasDisparityBS, ItemMADRanking, ItemMADRating, UserMADRanking, UserMADRating
   - **Multi-objective** (2): EuclideanDistance, Hypervolume

   Each metric entry on the family sub-page must include: (a) a brief scientific description, (b) the exact mathematical formula using LaTeX `.. math::` blocks, (c) the class signature with type hints (pulled from autodoc), and (d) a YAML configuration snippet showing how to enable the metric. Each family sub-page must also include a summary table with columns: Metric Name, Description, Type (Top-K / Global), and a link to the corresponding API Reference autoclass page (Task 8). Additionally, retain and update the existing overview page (`evaluation/metric.rst`) documenting evaluation modes (Full vs. Sampled), per-user analysis capabilities, and configuration examples — and the existing pages for sampled evaluation (`evaluation/guide/sample.rst`), block evaluation (`evaluation/guide/block.rst`), statistical significance testing (`evaluation/stat.rst`), and custom metric implementation guide (`evaluation/implement.rst`).

7. **Recommenders Taxonomy.** Restructure the recommenders documentation (`recommenders/`) to follow the pattern established by Elliot's `elliot/docs/source/guide/recommenders.rst`. Create a master index page (`recommenders/index.rst`) with a `toctree` linking to family sub-pages. For each family, create (or update the existing) sub-pages using the Elliot pattern — a Summary section with `.. py:module::` + `.. autosummary::` block, followed by individual model entries with `.. autoclass:: ModelName :show-inheritance:`. The families and their models are:
   - **Unpersonalized** (3): Pop, Random, ProxyRecommender — but move the detailed ProxyRecommender documentation to the cross-cutting page created in Task 5, leaving only a brief entry with a cross-reference here.
   - **Content-Based** (1): VSM
   - **Collaborative Filtering / Autoencoder** (7): EASE, ELSA, CDAE, MultiVAE, MultiDAE, MacridVAE, SANSA
   - **Collaborative Filtering / Graph-Based** (16): LightGCN, NGCF, DGCF, EGCF, ESIGCF, GCMC, LightCCF, LightGCL, LightGCN++, LightGODE, MixRec, RP3Beta, SGCL, SGL, UltraGCN, XSimGCL
   - **Collaborative Filtering / KNN** (2): ItemKNN, UserKNN
   - **Collaborative Filtering / Latent Factor** (4): BPR, FISM, Slim, ADMMSlim
   - **Collaborative Filtering / Neural** (2): NeuMF, ConvNCF
   - **Context-Aware** (8): AFM, DCN, DCNv2, DeepFM, FM, NFM, WideAndDeep, xDeepFM
   - **Sequential** (11): BERT4Rec, Caser, CORE, FOSSIL, GRU4Rec, gSASRec, LightSANs, LinRec, NARM, SASRec
   - **Hybrid** (4): AddEASE, CEASE, AttributeItemKNN, AttributeUserKNN

   Each model entry must include: (a) a brief description and theoretical background, (b) a link to the reference paper, (c) the class signature with key hyperparameters (pulled from autodoc), (d) the loss function or objective in LaTeX where applicable, and (e) a YAML configuration snippet showing how to configure the model. Also retain and update the existing recommender guides (`recommenders/guide/index.rst`, `recommenders/guide/classic.rst`, `recommenders/guide/iterative.rst`).

8. **API Reference via Autodocs.** Restructure the API reference section (`api_reference/`) to use Sphinx autodoc-generated pages instead of the current manually written ones. The API reference must be the authoritative, auto-generated documentation for every public class and method in WarpRec. Create the following sub-pages:
   - `api_reference/metrics.rst`: Use `.. automodule::` or `.. autoclass::` with `:show-inheritance:` and `:members:` for every metric class (39 metrics + `BaseMetric` and any intermediate abstract classes). Organize by family with clear section headings.
   - `api_reference/models.rst`: Use `.. autoclass::` with `:show-inheritance:` and `:members:` for every recommender class (56 models + base classes: `Recommender`, `IterativeRecommender`, `ItemSimRecommender`, and any mixins). Organize by family.
   - `api_reference/data_engine.rst`: Autodoc for Reader, Writer, Dataset, Filtering, and Splitting classes. Document how Narwhals is used to abstract Pandas/Polars/Spark dataframes.
   - `api_reference/pipelines.rst`: Autodoc for `design_pipeline`, `train_pipeline`, and `eval_pipeline` functions.
   - `api_reference/evaluation.rst`: Autodoc for `Evaluator`, `compute_paired_statistical_test`, and related evaluation infrastructure.

   Self-ask and resolve the following critical questions:
   - Napoleon processes docstrings before Sphinx sees them. The WarpRec docstrings mix Google-style parameter sections with `$$...$$` LaTeX blocks, inline `$...$`, reStructuredText links, and ASCII art tensor examples. Verify that each of these content types renders correctly through the Napoleon → autodoc → mathjax pipeline. If any content type is mangled, document the workaround (e.g., using `.. raw::` directives, custom Napoleon sections, or `napoleon_custom_sections` configuration).
   - Some docstrings contain reStructuredText inline links (`` `paper <url>`_ ``) inside Napoleon-parsed sections like `Args:`. Verify these are not misinterpreted as type annotations by Napoleon.
   - ASCII art tables in docstrings (e.g., in `nDCG`, `ARP`) should be rendered as literal/code blocks in the autodoc output. Verify they are not reformatted or collapsed.
   - The `@metric_registry.register("nDCG")` and `@model_registry.register(name="LightGCN")` decorators must be transparent to autodoc — verify they do not hide the class from documentation generation or alter the class signature.

9. **Advanced Features & USPs.** Write the advanced features documentation (`advanced/`) covering:
   - **Green AI Profiling** (`advanced/green_ai.rst`): Document how to enable CodeCarbon tracking via the dashboard configuration. Show an example of the output metrics (Emissions, CPU/GPU Power, RAM Energy) and explain how researchers can use this for sustainable AI reporting in their papers.
   - **Agentic AI & MCP** (`advanced/agentic_ai.rst`): Write a detailed guide on the Model Context Protocol integration. Provide a mock dialogue/code example showing an LLM (like Claude) calling the `WarpRec_SASRec.recommend` tool and reasoning over the output. Document the REST API endpoints and the MCP server configuration.
   - **Statistical Rigor** (`advanced/statistical_rigor.rst`): Document the hypothesis testing suite. Explain how to apply paired tests (Student's t-test, Wilcoxon) and independent tests (Mann-Whitney U), and how to enable Multiple Comparison Problem corrections (Bonferroni, Holm-Bonferroni, FDR). Provide YAML configuration examples and interpretation guidance for the output.

10. **Configuration Reference.** Write an exhaustive configuration reference (`configuration/`) documenting every YAML key accepted by WarpRec. Organize into sub-pages: `general.rst` (global settings: backend, random seeds, callbacks), `reader.rst` (data loading), `writer.rst` (result persistence), `filtering.rst` (13 filtering strategies), `splitter.rst` (6 splitting strategies), `models.rst` (model definitions and hyperparameter search spaces), `evaluation.rst` (metric configuration, evaluation strategies, cutoff values), and `dashboard.rst` (experiment tracking with Weights & Biases, MLflow, CodeCarbon). For each YAML key, provide: the key path, type, default value, description, and a usage example. Create a taxonomy table for the **19 filtering and splitting strategies** (13 Filtering: MinRating, UserAverage, ItemAverage, IterativeKCore, NRoundsKCore, UserMin, UserMax, ItemMin, ItemMax, UserHeadN, UserTailN, DropUser, DropItem; 6 Splitting: TemporalHoldout, TemporalLeaveKOut, TimestampSlicing, RandomHoldout, RandomLeaveKOut, KFoldCrossValidation) including mathematical definitions where applicable (e.g., $k$-core iterative filtering).

Important: We need to address each task sequentially, and I have to manually verify that each task has been fully addressed before moving on to the next one. Do not generate the entire documentation at once. First, output the proposed file structure (`toctree`) for the ReadTheDocs sidebar. Then generate the content for **Tasks 1 through 3** only. Stop and ask for my feedback and approval. Once approved, I will prompt you to continue with **Tasks 4 through 7**, and then **Tasks 8 through 10**. Most importantly, during the documentation writing process it is mandatory to self-ask critical questions and review design choices — particularly regarding Sphinx autodoc and Napoleon compatibility with the mixed-content docstrings (LaTeX, reStructuredText links, ASCII art, Google-style parameters). Do not use placeholders like "Insert code here" or "Add formula here." Extract the actual logic, parameters, and math from the provided paper and codebase.

## Tone, Style & Documentation Standards

The documentation must adhere to the following standards throughout every page and section:
- **Professional & Scientific:** Use precise academic terminology (e.g., "bipartite graph," "collaborative filtering," "hyperparameter optimization," "Type I error corrections"). Do not use casual language or marketing hyperbole.
- **Clear & Actionable:** Provide copy-pasteable code snippets for both local prototyping and distributed cluster execution. Every example must be complete and runnable.
- **Visually Structured:** Use reStructuredText tables, admonitions (`.. note::`, `.. warning::`, `.. tip::`, `.. important::`), and hierarchical headers. Use feature grids and summary tables to organize large amounts of information.
- **Mathematical Rigor:** Use `.. math::` directive blocks for displayed formulas and `:math:` inline roles for inline math. All metric definitions (e.g., nDCG, HR, Fairness metrics) and model objective functions (e.g., BPR loss, Matrix Factorization) must include their exact LaTeX formulas.
- **Autodoc & Napoleon Compliance:** All docstrings follow Google-style format. Class docstrings must include `Args:`, `Attributes:`, `Returns:`, `Raises:` as applicable. LaTeX within docstrings must use raw strings (`r"""`) and `$$...$$` blocks. The autodoc-generated API reference pages must render all content correctly.
- **Cross-References:** Use Sphinx `:ref:`, `:doc:`, and `:class:` roles to create navigable links between taxonomy pages, API reference pages, configuration reference, and pipeline documentation. Every model and metric mentioned in the taxonomy must link to its corresponding API reference autoclass page.
- **No Placeholders:** Every code snippet, formula, configuration example, class signature, and hyperparameter list must be real and extracted from the provided codebase and research paper. Never use "Insert code here," "TODO," or "Add formula here."

## Inputs for Processing

**1. The Research Paper:**
@Paper.md

**2. The Codebase:**
@Codebase
